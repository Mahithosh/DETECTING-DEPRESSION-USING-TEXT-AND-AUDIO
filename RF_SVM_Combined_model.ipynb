{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RF_SVM_Combined_model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPPiD40nkA+S+GRIcuBZijI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H9g24mb27Zck","executionInfo":{"status":"ok","timestamp":1626969947505,"user_tz":-330,"elapsed":24653,"user":{"displayName":"Sri Vishnuvardhan Reddy Akepati","photoUrl":"","userId":"12972633878670662839"}},"outputId":"43cb9162-023e-41d0-e7ac-df83493df3a8"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eCGs7iGQ93wG","executionInfo":{"status":"ok","timestamp":1626970009067,"user_tz":-330,"elapsed":2513,"user":{"displayName":"Sri Vishnuvardhan Reddy Akepati","photoUrl":"","userId":"12972633878670662839"}},"outputId":"6b236ac3-3b4f-440e-ce35-e1bc45ab5ce5"},"source":["import numpy as np\n","import nltk\n","import pandas as pd\n","from gensim.models.keyedvectors import KeyedVectors\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from sklearn.metrics import classification_report\n","import gc"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9z2ZQKit-ahU","executionInfo":{"status":"ok","timestamp":1626975601703,"user_tz":-330,"elapsed":242182,"user":{"displayName":"Sri Vishnuvardhan Reddy Akepati","photoUrl":"","userId":"12972633878670662839"}},"outputId":"3ad7dcdb-fed0-420b-f19f-3da685e22168"},"source":["print(\"Text Results: \")\n","\n","dataset1 = np.array(pd.read_csv('/content/drive/My Drive/Dataset/dev_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n","dataset2 = np.array(pd.read_csv('/content/drive/My Drive/Dataset/full_test_split.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n","dataset3 = np.array(pd.read_csv('/content/drive/My Drive/Dataset/train_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n","\n","\n","countPos = 0\n","dataset = np.concatenate((dataset1, np.concatenate((dataset2, dataset3))))\n","def checkPosNeg(dataset, index):\n","    for i in range(0, len(dataset)):\n","        if(dataset[i][0] == index):\n","            return dataset[i][1]\n","    return 0\n","\n","Data = []\n","Y = []\n","Data_test = []\n","Y_test = []\n","index = -1\n","for i in range(0, len(dataset3)):\n","    val = checkPosNeg(dataset, dataset3[i][0])\n","    if(val == 0 and countPos>38):\n","        continue\n","    Y.append(val)\n","    index+=1\n","    if(Y[index] == 0):\n","        countPos+=1\n","\n","    \n","    fileName = \"/content/drive/My Drive/Dataset/train_data/\" + str(int(dataset3[i][0])) + \"_TRANSCRIPT.csv\"\n","    Data.append(np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8'))[:, 2:4])\n","\n","\n","for i in range(0, len(dataset1)):\n","    val = checkPosNeg(dataset, dataset1[i][0])\n","    if(val == 0):\n","        continue\n","    Y.append(val)\n","    fileName = \"/content/drive/My Drive/Dataset/dev_data/\" + str(int(dataset1[i][0])) + \"_TRANSCRIPT.csv\"\n","    Data.append(np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8'))[:, 2:4])\n","    \n","for i in range(0, len(dataset2)):\n","    Y_test.append(checkPosNeg(dataset, dataset2[i][0]))\n","    fileName = \"/content/drive/My Drive/Dataset/test_data/\" + str(int(dataset2[i][0])) + \"_TRANSCRIPT.csv\"\n","    Data_test.append(np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8'))[:, 2:4])\n","Y = np.array(Y)\n","Data2 = []\n","\n","Data2_test = []\n","Y_test = np.array(Y_test)\n","\n","for i in range(0, len(Data)):\n","    script = []\n","    for k in range(1, len(Data[i])):\n","        if(Data[i][k][0] == \"Participant\"):\n","            script.append(Data[i][k][1])\n","    Data2.append(script)\n","    \n","for i in range(0, len(Data_test)):\n","    script = []\n","    for k in range(1, len(Data_test[i])):\n","        if(Data_test[i][k][0] == \"Participant\"):\n","            script.append(Data_test[i][k][1])\n","    Data2_test.append(script)\n","        \n","Data2 = np.array(Data2)\n","Data2_test = np.array(Data2_test)\n","\n","model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/Dataset/GoogleNews-vectors-negative300.bin.gz', binary=True)\n","\n","stop_words = set(stopwords.words('english'))\n","def remove_StopWOrds(sentence):\n","    filtered_sentence = [] \n","    for w in sentence: \n","        if w not in stop_words: \n","            filtered_sentence.append(w)\n","    \n","    return filtered_sentence\n","\n","def checkAcc(Y_pred, Y_test):\n","    correct = 0\n","    for i in range(len(Y_pred)):\n","        if(Y_pred[i] == Y_test[i]):\n","            correct+=1\n","    \n","    return float(correct)/len(Y_pred)\n","\n","\n","max_num_words = 15\n","max_num_sentence = 200\n","\n","#train_data\n","finalMatrix = np.zeros((Data2.shape[0], max_num_sentence, max_num_words))\n","print(finalMatrix.shape)\n","for k in range(Data2.shape[0]):\n","    for i in range(min(max_num_sentence, len(Data2[k]))):\n","    \ttry:\n","    \t  sentence = Data2[k][i].split(\" \")\n","    \texcept:\n","    \t  continue\n","    \tsentence = remove_StopWOrds(sentence)\n","    \tfor j in range(min(max_num_words, len(sentence))):\n","    \t\ttry:\n","    \t\t\tfinalMatrix[k][i][j] = np.average(np.array(model[sentence[j]]))\n","    \t\texcept:\n","    \t\t\tcontinue\n","X = np.zeros((len(Data2), max_num_sentence*max_num_words))\n","for i in range(len(finalMatrix)):\n","    X[i] = finalMatrix[i].flatten().reshape(1, -1)\n"," \n","#Test_data\n","finalMatrix_test = np.zeros((Data2_test.shape[0], max_num_sentence, max_num_words))\n","print(finalMatrix_test.shape)\n","for k in range(Data2_test.shape[0]):\n","    for i in range(min(max_num_sentence, len(Data2_test[k]))):\n","    \ttry:\n","    \t  sentence = Data2_test[k][i].split(\" \")\n","    \texcept:\n","    \t  continue\n","    \tsentence = remove_StopWOrds(sentence)\n","    \tfor j in range(min(max_num_words, len(sentence))):\n","    \t\ttry:\n","    \t\t\tfinalMatrix_test[k][i][j] = np.average(np.array(model[sentence[j]]))\n","    \t\texcept:\n","    \t\t\tcontinue\n","X_test = np.zeros((len(Data2_test), max_num_sentence*max_num_words))\n","for i in range(len(finalMatrix_test)):\n","    X_test[i] = finalMatrix_test[i].flatten().reshape(1, -1)\n","\n","Data2 = []\n","Data2_test = []\n","gc.collect()          \n","from sklearn.ensemble import RandomForestClassifier\n","clf1 = RandomForestClassifier()\n","clf1.fit(X,Y)\n","Y_pred_rf_1 = clf1.predict(X_test)\n","print(\"Random forests: \")\n","print(classification_report(Y_test,Y_pred_rf_1))\n","print(\"Train: \"+ str(clf1.score(X,Y)))\n","print(\"Test: \" + str(clf1.score(X_test,Y_test)))\n","\n","from sklearn.svm import SVC\n","\n","classifier = SVC(kernel = 'rbf', random_state = 0)\n","classifier.fit(X, Y)\n","\n","Y_pred_svm_1 = classifier.predict(X_test)\n","print(\"SVM: \")\n","print(classification_report(Y_test,Y_pred_svm_1))\n","print(\"Train: \"+ str(classifier.score(X,Y)))\n","print(\"Test: \" + str(classifier.score(X_test,Y_test)))\n","\n","y_pred_train_svm_1 = classifier.predict(X)\n","y_pred_train_rf_1 = clf1.predict(X)\n","\n","\n","#Audio Features \n","print(\"Audio Results: \")\n","import numpy as np\n","import pandas as pd \n","from sklearn.ensemble import RandomForestClassifier\n","def makedata(X):\n","    for i in range(X.shape[0]):\n","        if(X[i,1] == 0):\n","            X[i,0] = 0\n","            for j in range(7):\n","                X[i,j+1] = 0\n","    X = np.array(X)\n","    X = np.average(X, axis = 0)\n","    X = np.array(X.reshape(1, -1))\n","    return X\n"," \n","X_train = np.zeros((1,74))\n","Y_train = []\n","\n","X_test = np.zeros((1,74))\n","#Y_test = []\n","\n","for i in range(len(dataset2)):\n","    data = pd.read_csv('/content/drive/My Drive/Dataset/test_data/'+str(int(dataset2[i][0]))+\"_COVAREP.csv\",header = None)\n","    X_temp = data.iloc[:,:].values\n","    X_temp = makedata(X_temp)\n","    X_test = np.concatenate((X_test,X_temp),0)\n","    #Y_test.append(dataset2[i][1])\n","    \n","\n","X_test = np.delete(X_test,0,0)\n","#Y_test = np.array(Y_test)\n","count_0 = 0\n","count_1 = 0\n","for i in range(len(dataset3)):\n","    data = pd.read_csv('/content/drive/My Drive/Dataset/train_data/'+str(int(dataset3[i][0]))+\"_COVAREP.csv\",header = None)\n","    if(dataset3[i][1] == 0):\n","      count_0 +=1\n","      if(count_0<=39):\n","        X_temp = data.iloc[:,:].values\n","        X_temp = makedata(X_temp)\n","        X_train = np.concatenate((X_train,X_temp),0)\n","        Y_train.append(dataset3[i][1])\n","    else:\n","      count_1 +=1\n","      if(count_1<=28):\n","        X_temp = data.iloc[:,:].values\n","        X_temp = makedata(X_temp)\n","        X_train = np.concatenate((X_train,X_temp),0)\n","        Y_train.append(dataset3[i][1])\n","\n","count_0 = 0\n","count_1 = 0    \n","\n","for i in range(len(dataset1)):\n","    data = pd.read_csv('/content/drive/My Drive/Dataset/dev_data/'+str(int(dataset1[i][0]))+\"_COVAREP.csv\",header = None)\n","    if(dataset1[i][1] == 0):\n","      continue\n","    else:\n","      count_1+= 1\n","      if(count_1<=11):\n","        X_temp = data.iloc[:,:].values\n","        X_temp = makedata(X_temp)\n","        X_train = np.concatenate((X_train,X_temp),0)\n","        Y_train.append(dataset1[i][1])\n","    \n","\n","X_train = np.delete(X_train,0,0)\n","Y_train = np.array(Y_train)\n","clf1 = RandomForestClassifier()\n","clf1.fit(X_train,Y_train)\n","Y_pred_rf_2 = clf1.predict(X_test)\n","print(\"Random forests: \")\n","print(classification_report(Y_test,Y_pred_rf_2))\n","print(\"Train: \"+ str(clf1.score(X_train,Y_train)))\n","print(\"Test: \" + str(clf1.score(X_test,Y_test)))\n","\n","\n","from sklearn.svm import SVC\n","\n","\n","classifier = SVC(kernel = 'rbf', random_state = 0)\n","classifier.fit(X_train, Y_train)\n","Y_pred_svm_2 = classifier.predict(X_test)\n","print(\"SVM: \")\n","print(classification_report(Y_test,Y_pred_svm_2))\n","print(\"Train: \"+ str(classifier.score(X_train,Y_train)))\n","print(\"Test: \" + str(classifier.score(X_test,Y_test)))\n","\n","y_pred_train_svm_2 = classifier.predict(X_train)\n","y_pred_train_rf_2 = clf1.predict(X_train)\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Text Results: \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:67: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:68: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"],"name":"stderr"},{"output_type":"stream","text":["(37, 200, 15)\n","(16, 200, 15)\n","Random forests: \n","              precision    recall  f1-score   support\n","\n","           0       0.81      1.00      0.90        13\n","           1       0.00      0.00      0.00         3\n","\n","    accuracy                           0.81        16\n","   macro avg       0.41      0.50      0.45        16\n","weighted avg       0.66      0.81      0.73        16\n","\n","Train: 1.0\n","Test: 0.8125\n","SVM: \n","              precision    recall  f1-score   support\n","\n","           0       0.81      1.00      0.90        13\n","           1       0.00      0.00      0.00         3\n","\n","    accuracy                           0.81        16\n","   macro avg       0.41      0.50      0.45        16\n","weighted avg       0.66      0.81      0.73        16\n","\n","Train: 0.972972972972973\n","Test: 0.8125\n","Audio Results: \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["Random forests: \n","              precision    recall  f1-score   support\n","\n","           0       0.81      1.00      0.90        13\n","           1       0.00      0.00      0.00         3\n","\n","    accuracy                           0.81        16\n","   macro avg       0.41      0.50      0.45        16\n","weighted avg       0.66      0.81      0.73        16\n","\n","Train: 1.0\n","Test: 0.8125\n","SVM: \n","              precision    recall  f1-score   support\n","\n","           0       0.81      1.00      0.90        13\n","           1       0.00      0.00      0.00         3\n","\n","    accuracy                           0.81        16\n","   macro avg       0.41      0.50      0.45        16\n","weighted avg       0.66      0.81      0.73        16\n","\n","Train: 0.8378378378378378\n","Test: 0.8125\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g1FXXeYTRiWC","executionInfo":{"status":"ok","timestamp":1626975283005,"user_tz":-330,"elapsed":467,"user":{"displayName":"Sri Vishnuvardhan Reddy Akepati","photoUrl":"","userId":"12972633878670662839"}},"outputId":"7e16d840-38a2-4632-e6e7-aa5330ecf9c6"},"source":["# Combine\n","print(\"Combined Results: \") \n","X_2 = np.column_stack((y_pred_train_svm_1,y_pred_train_svm_2))\n","from sklearn.svm import SVC\n","classifier2 = SVC(kernel = 'rbf', random_state = 0, gamma = 'auto')\n","classifier2.fit(X_2, Y_train)\n","X_3 = np.column_stack(Y_pred_svm_1,Y_pred_svm_2))\n","y_pred_combine = classifier2.predict(X_3)\n","print(classification_report(Y_test, y_pred_combine))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Combined Results: \n","              precision    recall  f1-score   support\n","\n","           0       0.81      1.00      0.90        13\n","           1       0.00      0.00      0.00         3\n","\n","    accuracy                           0.81        16\n","   macro avg       0.41      0.50      0.45        16\n","weighted avg       0.66      0.81      0.73        16\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]}]}