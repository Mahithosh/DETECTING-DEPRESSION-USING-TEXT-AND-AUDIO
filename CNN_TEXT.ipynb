{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN_TEXT.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM9+B5s1VdSfavUOQygwgVg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t-XVHjhYTyQt","executionInfo":{"status":"ok","timestamp":1626975902914,"user_tz":-330,"elapsed":54724,"user":{"displayName":"Sri Vishnuvardhan Reddy Akepati","photoUrl":"","userId":"12972633878670662839"}},"outputId":"507cc7d3-4c32-4281-957b-7efb42e1e790"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X2l7JnZeUirN","executionInfo":{"status":"ok","timestamp":1626975933257,"user_tz":-330,"elapsed":3842,"user":{"displayName":"Sri Vishnuvardhan Reddy Akepati","photoUrl":"","userId":"12972633878670662839"}},"outputId":"db118ca7-4cbe-434c-9791-a2924c3d6e6d"},"source":["import nltk\n","nltk.download('stopwords')\n","import numpy as np\n","import pandas as pd\n","from gensim.models.keyedvectors import KeyedVectors\n","import gc\n","from keras.models import Sequential\n","from keras.layers import Conv2D\n","from keras.layers import MaxPooling2D\n","from keras.layers import Flatten\n","from keras.layers import Dense\n","from smart_open import open\n","from nltk.corpus import stopwords\n","from sklearn.metrics import classification_report\n","from keras.layers import Dropout\n","from matplotlib import pyplot as plt\n","from sklearn.metrics import classification_report"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0cg4TtUHXw0H","executionInfo":{"status":"ok","timestamp":1626978309489,"user_tz":-330,"elapsed":167807,"user":{"displayName":"Sri Vishnuvardhan Reddy Akepati","photoUrl":"","userId":"12972633878670662839"}},"outputId":"bf52403b-32da-4de6-c56d-de8561767a65"},"source":["dataset1 = np.array(pd.read_csv('/content/drive/My Drive/Dataset/dev_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n","dataset2 = np.array(pd.read_csv('/content/drive/My Drive/Dataset/full_test_split.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n","dataset3 = np.array(pd.read_csv('/content/drive/My Drive/Dataset/train_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n","\n","dataset = np.concatenate((dataset1, np.concatenate((dataset2, dataset3))))\n","\n","countPos = 0\n","\n","def checkPosNeg(dataset, index):\n","    for i in range(0, len(dataset)):\n","        if(dataset[i][0] == index):\n","            return dataset[i][1]\n","    return 0\n","\n","Data = []\n","Y = []\n","\n","countPos = 0\n","index = -1\n","Data_test = []\n","Y_test = []\n","for i in range(0, len(dataset3)):\n","    val = checkPosNeg(dataset, dataset3[i][0])\n","    Y.append(val)\n","    try:\n","      fileName = \"/content/drive/My Drive/Dataset/train_data/\" + str(int(dataset3[i][0])) + \"_TRANSCRIPT.csv\"\n","      Data.append(np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8', engine='python'))[:, 2:4])\n","    except Exception as e:\n","       print(e)\n","for i in range(0, len(dataset1)):\n","    val = checkPosNeg(dataset, dataset1[i][0])\n","    Y.append(val)\n","    try:\n","      fileName = \"/content/drive/My Drive/Dataset/dev_data/\" + str(int(dataset1[i][0])) + \"_TRANSCRIPT.csv\"\n","      Data.append(np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8', engine='python'))[:, 2:4])\n","    except Exception as e:\n","       print(e)\n","for i in range(0, len(dataset2)):\n","    Y_test.append(checkPosNeg(dataset, dataset2[i][0]))\n","    try:\n","      fileName = \"/content/drive/My Drive/Dataset/test_data/\" + str(int(dataset2[i][0])) + \"_TRANSCRIPT.csv\"\n","      Data_test.append(np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8', engine='python'))[:, 2:4])\n","    except Exception as e:\n","       print(e)\n","\n","    \n","Y = np.array(Y)\n","Data2 = []\n","\n","Data2_test = []\n","Y_test = np.array(Y_test)       \n","for i in range(0, len(Data)):\n","    script = []\n","    for k in range(1, len(Data[i])):\n","        if(Data[i][k][0] == \"Participant\"):\n","            script.append(Data[i][k][1])\n","    Data2.append(script)\n","    \n","for i in range(0, len(Data_test)):\n","    script = []\n","    for k in range(1, len(Data_test[i])):\n","        if(Data_test[i][k][0] == \"Participant\"):\n","            script.append(Data_test[i][k][1])\n","    Data2_test.append(script)\n","\n","Data = []\n","Data_test = []\n","gc.collect()        \n","Data2 = np.array(Data2)\n","Data2_test = np.array(Data2_test)\n","\n","model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/Dataset/GoogleNews-vectors-negative300.bin.gz', binary=True)\n","stop_words = set(stopwords.words('english'))\n","\n","\n","def Thresholding(Y_pred, threshold):\n","  Y_pred2 = []\n","  for i in range(len(Y_pred)):\n","    if(Y_pred[i] < threshold):\n","      Y_pred2.append(0)\n","    else:\n","      Y_pred2.append(1)\n","\n","  return np.array(Y_pred2)\n","def remove_StopWords(sentence):\n","    filtered_sentence = [] \n","    for w in sentence: \n","        if w not in stop_words: \n","            filtered_sentence.append(w)\n","    \n","    return filtered_sentence\n","\n","def checkAcc(Y_pred, Y_test):\n","    correct = 0\n","    for i in range(len(Y_pred)):\n","        if(Y_pred[i] == Y_test[i]):\n","            correct+=1\n","    \n","    return float(correct)/len(Y_pred)\n","def upsample(X_train,Y_train):\n","  X_train_0 = X_train[Y_train==0]\n","  X_train_1 = X_train[Y_train==1]\n","\n","  Y_train_1 = Y_train[Y_train==1]\n","  size = X_train_0.shape[0] - X_train_1.shape[0]\n","  X = []\n","  Y = []\n","  X_train = list(X_train)\n","  Y_train = list(Y_train)\n","  while(size>0):\n","    size -= 1\n","    index = np.random.randint(0,X_train_1.shape[0]-1)\n","    leave_index = np.random.randint(0,len(X_train)-1)\n","    X_add = X_train_1[index]\n","    X_leave = X_train[leave_index]\n","\n","    Y_add = Y_train_1[index]\n","    Y_leave = Y_train[leave_index]\n","\n","    X_train[leave_index] = X_add\n","    X_train.append(X_leave)\n","\n","    Y_train[leave_index] = Y_add\n","    Y_train.append(Y_leave)\n","\n","\n","  X_train = np.array(X_train)\n","  Y_train = np.array(Y_train)\n","  return X_train,Y_train\n","\n","\n","max_num_words = 20\n","max_num_sentence = 250      \n","#train_data\n","finalMatrix = np.zeros((Data2.shape[0], max_num_sentence, max_num_words, 300))\n","max_length_sent = 0\n","sent = \"\"\n","for k in range(Data2.shape[0]):\n","    if(max_length_sent < len(Data2[k])):\n","      max_length_sent = len(Data2[k])\n","      sent = Data2[k]\n","    for i in range(min(max_num_sentence, len(Data2[k]))):\n","    \ttry:\n","    \t  sentence = Data2[k][i].split(\" \")\n","    \texcept:\n","    \t  continue\n","    \tsentence = remove_StopWords(sentence)\n","    \tfor j in range(min(max_num_words, len(sentence))):\n","    \t\ttry:\n","    \t\t  word = sentence[j]\n","    \t\t  # print(\"Before\", word)\n","    \t\t  if(word[0] == '<'):\n","    \t\t    if(word.find('>')!=-1):\n","    \t\t      word = word[1:-1]\n","    \t\t    else:\n","    \t\t      word = word[1:]\n","    \t\t  else:\n","    \t\t    if(word.find('>')!=-1):\n","    \t\t      word = word[0:-1]\n","    \t\t  finalMatrix[k][i][j] = np.array(model[word])\n","    \t\texcept Exception as e:\n","    \t\t\tcontinue\n","#Test_data\n","max_length_sent = 0\n","finalMatrix_test = np.zeros((Data2_test.shape[0], max_num_sentence, max_num_words, 300))\n","# print(finalMatrix_test.shape)\n","for k in range(Data2_test.shape[0]):\n","    if(max_length_sent < len(Data2_test[k])):\n","      max_length_sent = len(Data2_test[k])\n","      sent = Data2_test[k]\n","    for i in range(min(max_num_sentence, len(Data2_test[k]))):\n","    \ttry:\n","    \t  sentence = Data2_test[k][i].split(\" \")\n","    \texcept:\n","    \t  continue\n","    \tsentence = remove_StopWords(sentence)\n","    \tfor j in range(min(max_num_words, len(sentence))):\n","    \t\ttry:\n","    \t\t  word = sentence[j]\n","    \t\t  if(word[0] == '<'):\n","    \t\t    if(word.find('>')!=-1):\n","    \t\t      word = word[1:-1]\n","    \t\t    else:\n","    \t\t      word = word[1:]\n","    \t\t  else:\n","    \t\t    if(word.find('>')!=-1):\n","    \t\t      word = word[0:-1]\n","    \t\t  finalMatrix_test[k][i][j] = np.array(model[word])\n","    \t\texcept Exception as e:\n","    \t\t\tcontinue\n","\n","Data2 = []\n","Data2_test = []\n","model = []\n","stop_words = []\n","gc.collect()\n","\n","finalMatrix, Y = upsample(finalMatrix,Y)\n","finalMatrix_test, Y_test = upsample(finalMatrix_test,Y_test)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:69: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NJ8X-5N2d0dT","executionInfo":{"status":"ok","timestamp":1626981264710,"user_tz":-330,"elapsed":55333,"user":{"displayName":"Sri Vishnuvardhan Reddy Akepati","photoUrl":"","userId":"12972633878670662839"}},"outputId":"1acf3715-ef4b-46df-9adb-94ff55e5f51e"},"source":["classifier = Sequential()\n","classifier.add(Conv2D(150, (1, 5), input_shape = (finalMatrix.shape[1], finalMatrix.shape[2], finalMatrix.shape[3]), activation = 'relu', data_format=\"channels_last\"))\n","classifier.add(MaxPooling2D(pool_size = (1, 3)))\n","classifier.add(Conv2D(75, (1, 3), activation = 'relu', data_format=\"channels_last\"))\n","classifier.add(MaxPooling2D(pool_size = (1, 2)))\n","classifier.add(Flatten())\n","# Step 4 - Full connection\n","classifier.add(Dense(units = 128, activation = 'relu'))\n","classifier.add(Dense(units = 1, activation = 'sigmoid'))\n","\n","classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","class_weight = {0: 0.5, 1: 0.6}\n","classifier.fit(finalMatrix, Y, epochs = 10, class_weight=class_weight)\n","Y_Pred = Thresholding(classifier.predict(finalMatrix_test), 0.5)  \n","\n","print(classification_report(Y_test, Y_Pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","3/3 [==============================] - 6s 2s/step - loss: 0.3075 - accuracy: 0.6779\n","Epoch 2/10\n","3/3 [==============================] - 5s 2s/step - loss: 0.0357 - accuracy: 0.9943\n","Epoch 3/10\n","3/3 [==============================] - 5s 2s/step - loss: 0.0188 - accuracy: 0.9904\n","Epoch 4/10\n","3/3 [==============================] - 5s 2s/step - loss: 1.9054e-05 - accuracy: 1.0000\n","Epoch 5/10\n","3/3 [==============================] - 5s 2s/step - loss: 5.4650e-06 - accuracy: 1.0000\n","Epoch 6/10\n","3/3 [==============================] - 5s 2s/step - loss: 3.4225e-04 - accuracy: 1.0000\n","Epoch 7/10\n","3/3 [==============================] - 5s 2s/step - loss: 4.4640e-05 - accuracy: 1.0000\n","Epoch 8/10\n","3/3 [==============================] - 5s 2s/step - loss: 5.4932e-06 - accuracy: 1.0000\n","Epoch 9/10\n","3/3 [==============================] - 5s 2s/step - loss: 2.6666e-06 - accuracy: 1.0000\n","Epoch 10/10\n","3/3 [==============================] - 5s 2s/step - loss: 1.0950e-06 - accuracy: 1.0000\n","              precision    recall  f1-score   support\n","\n","           0       0.50      1.00      0.67        13\n","           1       0.00      0.00      0.00        13\n","\n","    accuracy                           0.50        26\n","   macro avg       0.25      0.50      0.33        26\n","weighted avg       0.25      0.50      0.33        26\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]}]}